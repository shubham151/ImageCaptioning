{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubham151/ImageCaptioning/blob/main/Ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "to_tensor = transforms.ToTensor()\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm.auto import tqdm\n",
        "import itertools\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "rLJjxrfSeT4U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "s40hwaA7_Sax"
      },
      "outputs": [],
      "source": [
        "# !pip install torch torchvision\n",
        "# # !pip install torch==1.10.0 torchvision==0.11.0\n",
        "# !nvcc --version\n",
        "\n",
        "\n",
        "# # import torch\n",
        "# print(torch.__version__)\n",
        "# import torchvision\n",
        "# print(torchvision.__version__)\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm.auto import tqdm\n",
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "to_tensor = transforms.ToTensor()\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "!unzip -q \"/content/drive/MyDrive/Ai_proj/seg_pred.zip\"\n",
        "!unzip -q \"/content/drive/MyDrive/Ai_proj/seg_test.zip\"\n",
        "!unzip -q \"/content/drive/MyDrive/Ai_proj/seg_train.zip\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krTDORyf_qmf",
        "outputId": "77baca89-6374-4f71-f42b-1c200fedf74d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "replace seg_pred/10004.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace seg_test/buildings/20057.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace seg_train/buildings/0.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "traindata_path = \"/content/seg_train\"\n",
        "valdata_path = \"/content/seg_test\"\n",
        "testset_path = \"/content/seg_pred\""
      ],
      "metadata": {
        "id": "8X5wmAalANP_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p = 0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize([50,50], antialias = True),\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize([50,50], antialias = True)\n",
        "])\n",
        "\n",
        "train_data = ImageFolder(traindata_path, transform=train_transforms)\n",
        "val_data = ImageFolder(valdata_path, transform = val_transforms)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64,num_workers = 4, shuffle=True, drop_last = True)\n",
        "val_loader = DataLoader(val_data, batch_size=500, drop_last = False)\n"
      ],
      "metadata": {
        "id": "WvU7ocbg_ohR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlexibleCNN(nn.Module):\n",
        "    def __init__(self, num_blocks, channels, kernel_size=3, activation= nn.ReLU(), downsampling='maxpool', dropout_prob = 0.5):\n",
        "        super(FlexibleCNN, self).__init__()\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "        if isinstance(channels, list):\n",
        "            assert len(channels) == num_blocks\n",
        "        else:\n",
        "            channels = [channels]*num_blocks\n",
        "\n",
        "        channels = [3] + channels #at the beginning we have only 3\n",
        "\n",
        "        for i in range(num_blocks):\n",
        "            block = self.build_cnn_block(channels[i],channels[i+1], kernel_size, activation, downsampling, dropout_prob)\n",
        "            self.blocks.append(block)\n",
        "\n",
        "        self.global_avg_pooling = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(channels[-1], 6)  # 6 output classes\n",
        "\n",
        "    def build_cnn_block(self, in_channels, out_channels, kernel_size, activation, downsampling, dropout_prob):\n",
        "        layers = []\n",
        "\n",
        "        stride = 2 if downsampling == \"stride\" else 1\n",
        "        # Batch normalization\n",
        "        layers.append(nn.BatchNorm2d(in_channels))\n",
        "\n",
        "        # Convolutional layer\n",
        "        layers.append(nn.Conv2d(in_channels= in_channels, out_channels= out_channels, kernel_size=kernel_size, padding=1, stride = stride))\n",
        "\n",
        "        # Dropout layer\n",
        "        layers.append(nn.Dropout2d(p = dropout_prob))\n",
        "\n",
        "        layers.append(activation)\n",
        "\n",
        "        # Downsampling technique\n",
        "        if downsampling == 'maxpool':\n",
        "            layers.append(nn.MaxPool2d(2, 2))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.global_avg_pooling(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "eG2ZWXYWCPf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResCNN(nn.Module):\n",
        "    def __init__(self, num_blocks, channels, kernel_size=3, activation= nn.ReLU(), downsampling='maxpool', dropout_prob = 0.5, padding = 1):\n",
        "        super(ResCNN, self).__init__()\n",
        "\n",
        "        self.res_blocks = nn.ModuleList()\n",
        "        self.embed_layer = nn.Conv2d(3, channels,kernel_size = 1)\n",
        "        self.activation = activation\n",
        "        for i in range(num_blocks):\n",
        "            res_block = self.build_res_block(channels,channels, kernel_size, activation, downsampling, dropout_prob, padding)\n",
        "            self.res_blocks.append(res_block)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2,2)\n",
        "        self.global_avg_pooling = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(channels, 6)  # 6 output classes\n",
        "\n",
        "    def build_res_block(self, in_channels, out_channels, kernel_size, activation, downsampling, dropout_prob, padding):\n",
        "        layers = []\n",
        "        layers.append(nn.BatchNorm2d(in_channels))\n",
        "        layers.append(nn.Conv2d(in_channels= in_channels, out_channels= out_channels, kernel_size=kernel_size, padding=padding, stride = 1))\n",
        "        layers.append(nn.Dropout2d(p = dropout_prob))\n",
        "        layers.append(activation)\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed_layer(x)\n",
        "        x = self.activation(x)\n",
        "        for block in self.res_blocks:\n",
        "            y = block(x)\n",
        "            x = x + y\n",
        "            x = self.maxpool(x)\n",
        "\n",
        "        x = self.global_avg_pooling(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "num_blocks = 2\n",
        "channels = 256\n",
        "kernel_size = 3\n",
        "padding = 1\n",
        "activation = nn.ReLU()\n",
        "dropout_prob = 0.3"
      ],
      "metadata": {
        "id": "FVX50f1ZCWya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best_result = max(results, key=lambda x: x['final_val_accuracy'])\n",
        "print(\"\\nBest result:\", \"Best result: {'num_blocks': 3, 'channels': [128, 256, 512], 'kernel_size': 3, 'activation': ReLU(), 'downsampling': 'maxpool', 'dropout_prob': 0.3, 'final_train_loss': 0.4787631332874298, 'final_train_accuracy': 0.827917913638307, 'final_val_loss': 0.43733835220336914, 'final_val_accuracy': 0.847}\")"
      ],
      "metadata": {
        "id": "wYXMERrTChQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WARNING : This cell is VERY long to run (~2 hours). The output has been copy-pasted below for convenience. Feel free to skip / comment it.\n",
        "\n",
        "device = \"cuda\"\n",
        "epochs = 10\n",
        "\n",
        "num_blocks_values = [3]\n",
        "channels_values = [64, 128, 256]\n",
        "kernel_size_values = [3]\n",
        "activation_values = [nn.ReLU(), nn.Sigmoid()]\n",
        "downsampling_values = ['maxpool', 'stride']\n",
        "dropout_prob_values = [0.3, 0.5]\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "best_model = None\n",
        "\n",
        "grid_search = itertools.product(num_blocks_values, channels_values, kernel_size_values, activation_values, downsampling_values, dropout_prob_values)\n",
        "\n",
        "def train_and_evaluate_model(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        train_accuracy = correct / total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_losses.append(loss.item())\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = np.mean(val_losses)\n",
        "        val_accuracy = correct / total\n",
        "\n",
        "        #print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\")\n",
        "\n",
        "\n",
        "    return model, train_loss, train_accuracy, val_loss, val_accuracy\n",
        "\n",
        "for num_blocks, channel, kernel_size, activation, downsampling, dropout_prob in grid_search:\n",
        "    model = ResCNN(num_blocks, channels, kernel_size, activation, downsampling, dropout_prob)\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    trained_model, train_loss, train_accuracy, val_loss, val_accuracy = train_and_evaluate_model(model, train_loader, val_loader, criterion, optimizer, epochs, device)\n",
        "\n",
        "    print(f\"num_blocks: {num_blocks}, channel: {channel}, kernel_size: {kernel_size}, activation: {activation}, downsampling: {downsampling}, dropout_prob: {dropout_prob}, Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\")\n",
        "\n",
        "    # Compare and update the best model\n",
        "    if val_accuracy > best_accuracy:\n",
        "        best_accuracy = val_accuracy\n",
        "        best_params = {'num_blocks': num_blocks, 'channel': channel, 'kernel_size': kernel_size, 'activation': activation, 'downsampling': downsampling, 'dropout_prob': dropout_prob}\n",
        "        best_model = trained_model\n",
        "\n",
        "print(f\"Best parameters: {best_params} with accuracy {best_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctOSCe4qCrBm",
        "outputId": "ae842e1e-a077-4b18-bfcb-8b3fa0880612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_blocks: 3, channel: 64, kernel_size: 3, activation: ReLU(), downsampling: maxpool, dropout_prob: 0.3, Train Loss: 0.641472888183376, Train Accuracy: 0.7664098173515982, Val Loss: 0.6117745985587438, Val Accuracy: 0.7786666666666666\n",
            "num_blocks: 3, channel: 64, kernel_size: 3, activation: ReLU(), downsampling: maxpool, dropout_prob: 0.5, Train Loss: 0.7652883809995433, Train Accuracy: 0.7180365296803652, Val Loss: 0.7157183388868967, Val Accuracy: 0.7446666666666667\n",
            "num_blocks: 3, channel: 64, kernel_size: 3, activation: ReLU(), downsampling: stride, dropout_prob: 0.3, Train Loss: 0.6443619552540453, Train Accuracy: 0.7685502283105022, Val Loss: 0.6104919662078222, Val Accuracy: 0.7816666666666666\n",
            "num_blocks: 3, channel: 64, kernel_size: 3, activation: ReLU(), downsampling: stride, dropout_prob: 0.5, Train Loss: 0.7684658118034606, Train Accuracy: 0.7169663242009132, Val Loss: 0.6985585490862528, Val Accuracy: 0.7536666666666667\n",
            "num_blocks: 3, channel: 64, kernel_size: 3, activation: Sigmoid(), downsampling: maxpool, dropout_prob: 0.3, Train Loss: 0.8666215304914675, Train Accuracy: 0.6865724885844748, Val Loss: 0.8042091677586237, Val Accuracy: 0.708\n",
            "num_blocks: 3, channel: 64, kernel_size: 3, activation: Sigmoid(), downsampling: maxpool, dropout_prob: 0.5, Train Loss: 0.9741541933795633, Train Accuracy: 0.6443350456621004, Val Loss: 0.9932134399811426, Val Accuracy: 0.6326666666666667\n",
            "num_blocks: 3, channel: 64, kernel_size: 3, activation: Sigmoid(), downsampling: stride, dropout_prob: 0.3, Train Loss: 0.8639382219205708, Train Accuracy: 0.6904965753424658, Val Loss: 0.81169460217158, Val Accuracy: 0.7076666666666667\n",
            "num_blocks: 3, channel: 64, kernel_size: 3, activation: Sigmoid(), downsampling: stride, dropout_prob: 0.5, Train Loss: 0.9831553253953316, Train Accuracy: 0.6379851598173516, Val Loss: 1.0585947036743164, Val Accuracy: 0.603\n",
            "num_blocks: 3, channel: 128, kernel_size: 3, activation: ReLU(), downsampling: maxpool, dropout_prob: 0.3, Train Loss: 0.6310566641968679, Train Accuracy: 0.7732591324200914, Val Loss: 0.5805678268273672, Val Accuracy: 0.803\n",
            "num_blocks: 3, channel: 128, kernel_size: 3, activation: ReLU(), downsampling: maxpool, dropout_prob: 0.5, Train Loss: 0.7653050732939211, Train Accuracy: 0.7159674657534246, Val Loss: 0.6848987638950348, Val Accuracy: 0.763\n",
            "num_blocks: 3, channel: 128, kernel_size: 3, activation: ReLU(), downsampling: stride, dropout_prob: 0.3, Train Loss: 0.635851261142182, Train Accuracy: 0.7702625570776256, Val Loss: 0.6160745223363241, Val Accuracy: 0.7876666666666666\n",
            "num_blocks: 3, channel: 128, kernel_size: 3, activation: ReLU(), downsampling: stride, dropout_prob: 0.5, Train Loss: 0.7745485052670518, Train Accuracy: 0.7100456621004566, Val Loss: 0.6982271025578181, Val Accuracy: 0.76\n",
            "num_blocks: 3, channel: 128, kernel_size: 3, activation: Sigmoid(), downsampling: maxpool, dropout_prob: 0.3, Train Loss: 0.8674834977546239, Train Accuracy: 0.6903538812785388, Val Loss: 0.8199987163146337, Val Accuracy: 0.6903333333333334\n",
            "num_blocks: 3, channel: 128, kernel_size: 3, activation: Sigmoid(), downsampling: maxpool, dropout_prob: 0.5, Train Loss: 0.9909132539409481, Train Accuracy: 0.63363299086758, Val Loss: 1.0084582716226578, Val Accuracy: 0.6256666666666667\n",
            "num_blocks: 3, channel: 128, kernel_size: 3, activation: Sigmoid(), downsampling: stride, dropout_prob: 0.3, Train Loss: 0.8830074036502402, Train Accuracy: 0.68050799086758, Val Loss: 0.8284180661042532, Val Accuracy: 0.6973333333333334\n",
            "num_blocks: 3, channel: 128, kernel_size: 3, activation: Sigmoid(), downsampling: stride, dropout_prob: 0.5, Train Loss: 0.9870440616999587, Train Accuracy: 0.6396261415525114, Val Loss: 1.0820516000191371, Val Accuracy: 0.611\n",
            "num_blocks: 3, channel: 256, kernel_size: 3, activation: ReLU(), downsampling: maxpool, dropout_prob: 0.3, Train Loss: 0.6412390414710458, Train Accuracy: 0.7686929223744292, Val Loss: 0.5909518549839655, Val Accuracy: 0.798\n",
            "num_blocks: 3, channel: 256, kernel_size: 3, activation: ReLU(), downsampling: maxpool, dropout_prob: 0.5, Train Loss: 0.7730580211774399, Train Accuracy: 0.7178938356164384, Val Loss: 0.7010803818702698, Val Accuracy: 0.754\n",
            "num_blocks: 3, channel: 256, kernel_size: 3, activation: ReLU(), downsampling: stride, dropout_prob: 0.3, Train Loss: 0.63701471455021, Train Accuracy: 0.7735445205479452, Val Loss: 0.5904984325170517, Val Accuracy: 0.8036666666666666\n",
            "num_blocks: 3, channel: 256, kernel_size: 3, activation: ReLU(), downsampling: stride, dropout_prob: 0.5, Train Loss: 0.7695295037744252, Train Accuracy: 0.7171803652968036, Val Loss: 0.6892409125963846, Val Accuracy: 0.7626666666666667\n",
            "num_blocks: 3, channel: 256, kernel_size: 3, activation: Sigmoid(), downsampling: maxpool, dropout_prob: 0.3, Train Loss: 0.8647954153687987, Train Accuracy: 0.6857163242009132, Val Loss: 0.8033415178457896, Val Accuracy: 0.6996666666666667\n",
            "num_blocks: 3, channel: 256, kernel_size: 3, activation: Sigmoid(), downsampling: maxpool, dropout_prob: 0.5, Train Loss: 0.9781343939641839, Train Accuracy: 0.63712899543379, Val Loss: 1.0371227065722148, Val Accuracy: 0.62\n",
            "num_blocks: 3, channel: 256, kernel_size: 3, activation: Sigmoid(), downsampling: stride, dropout_prob: 0.3, Train Loss: 0.8693071426866261, Train Accuracy: 0.6862157534246576, Val Loss: 0.8038598398367564, Val Accuracy: 0.707\n",
            "num_blocks: 3, channel: 256, kernel_size: 3, activation: Sigmoid(), downsampling: stride, dropout_prob: 0.5, Train Loss: 0.983735236130893, Train Accuracy: 0.633490296803653, Val Loss: 1.0408017685015996, Val Accuracy: 0.6103333333333333\n",
            "Best parameters: {'num_blocks': 3, 'channel': 256, 'kernel_size': 3, 'activation': ReLU(), 'downsampling': 'stride', 'dropout_prob': 0.3} with accuracy 0.8036666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the best model using pickle\n",
        "with open('best_model.pkl', 'wb') as file:\n",
        "    pickle.dump(best_model, file)"
      ],
      "metadata": {
        "id": "Bt1w1I1gwgZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alexnet**"
      ],
      "metadata": {
        "id": "Oo2CfAzubjwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "9I9jW6uybg0Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "38c49312-7090-4d6d-9ff5-bb04dfa5488d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d29fba5f7a75>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transform = transforms.Compose([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCenterCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_data = ImageFolder(traindata_path, transform=train_transforms)\n",
        "val_data = ImageFolder(valdata_path, transform = val_transforms)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64,num_workers = 4, shuffle=True, drop_last = True)\n",
        "val_loader = DataLoader(val_data, batch_size=500, drop_last = False)\n"
      ],
      "metadata": {
        "id": "4m9lelEYCzwi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "f149b12c-7532-49e1-8adb-a48bef18f66e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9c371700ea2f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_transforms = transforms.Compose([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCenterCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "3YzvbkVwbsVp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5795e16d-e2c3-4a1d-e308-daba189fe1e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 90493468.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the AlexNet model\n",
        "model = models.alexnet(pretrained=False)  # Set pretrained=True if you want to use a pre-trained model\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 15\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "c--1q1gpbu2u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "f76320bd-5540-4c74-9cc5-4fb87bed0a21"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d66fc7f6ce48>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the AlexNet model\n",
        "model = models.alexnet(pretrained=False)  # Set pretrained=True if you want to use a pre-trained model\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')\n",
        "\n",
        "    # Calculate and print epoch accuracy\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the model after epoch {epoch+1}: {epoch_accuracy}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "Bu3eC1vYESK6",
        "outputId": "a75df3a1-2778-4237-800d-c0b936f49eee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-78ca751f9f15>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define the AlexNet model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malexnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set pretrained=True if you want to use a pre-trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define loss function and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QSFyPrMXGjvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "hsuifL84Zan-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the best model using pickle\n",
        "with open('alexnet_model.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "OmKl7cVJ_wfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dm1Evpq6Dkga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "GvOCKjpX_4xp"
      }
    }
  ]
}